{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"4\"></a>\n",
        "## 4 - Refresher on linear regression\n",
        "\n",
        "In this practice lab, you will fit the linear regression parameters $(w,b)$ to your dataset.\n",
        "- The model function for linear regression, which is a function that maps from `x` (city population) to `y` (your restaurant's monthly profit for that city) is represented as\n",
        "    $$f_{w,b}(x) = wx + b$$\n",
        "    \n",
        "\n",
        "- To train a linear regression model, you want to find the best $(w,b)$ parameters that fit your dataset.  \n",
        "\n",
        "    - To compare how one choice of $(w,b)$ is better or worse than another choice, you can evaluate it with a cost function $J(w,b)$\n",
        "      - $J$ is a function of $(w,b)$. That is, the value of the cost $J(w,b)$ depends on the value of $(w,b)$.\n",
        "  \n",
        "    - The choice of $(w,b)$ that fits your data the best is the one that has the smallest cost $J(w,b)$.\n",
        "\n",
        "\n",
        "- To find the values $(w,b)$ that gets the smallest possible cost $J(w,b)$, you can use a method called **gradient descent**.\n",
        "  - With each step of gradient descent, your parameters $(w,b)$ come closer to the optimal values that will achieve the lowest cost $J(w,b)$.\n",
        "  \n",
        "\n",
        "- The trained linear regression model can then take the input feature $x$ (city population) and output a prediction $f_{w,b}(x)$ (predicted monthly profit for a restaurant in that city)."
      ],
      "metadata": {
        "id": "DFDqAcnYMvLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notation\n",
        "Here is a summary of some of the notation you will encounter.  \n",
        "\n",
        "|General <img width=70/> <br />  Notation  <img width=70/> | Description<img width=350/>| Python (if applicable) |\n",
        "|: ------------|: ------------------------------------------------------------||\n",
        "| $a$ | scalar, non bold                                                      ||\n",
        "| $\\mathbf{a}$ | vector, bold                                                      ||\n",
        "| **Regression** |         |    |     |\n",
        "|  $\\mathbf{x}$ | Training Example feature values (in this lab - Size (1000 sqft))  | `x_train` |   \n",
        "|  $\\mathbf{y}$  | Training Example  targets (in this lab Price (1000s of dollars))  | `y_train`\n",
        "|  $x^{(i)}$, $y^{(i)}$ | $i_{th}$Training Example | `x_i`, `y_i`|\n",
        "| m | Number of training examples | `m`|\n",
        "|  $w$  |  parameter: weight                                 | `w`    |\n",
        "|  $b$           |  parameter: bias                                           | `b`    |     \n",
        "| $f_{w,b}(x^{(i)})$ | The result of the model evaluation at $x^{(i)}$ parameterized by $w,b$: $f_{w,b}(x^{(i)}) = wx^{(i)}+b$  | `f_wb` |\n"
      ],
      "metadata": {
        "id": "C1CmEot8HXj7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "k2t0yd8zHiUV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "taSG_wYmHChh",
        "outputId": "a556eece-90a4-4f11-bfaf-3a611f6a0697"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 3) (<ipython-input-1-846a72803d1c>, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-846a72803d1c>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    I'd like to connect to better understand your path and the role as well! Thanks!\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 3)\n"
          ]
        }
      ],
      "source": [
        "import math, copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('./deeplearning.mplstyle')\n",
        "from lab_utils_uni import plt_house_x, plt_contour_wgrad, plt_divergence, plt_gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model function\n",
        "\n",
        "<img align=\"left\" src=\"./images/C1_W1_L3_S1_model.png\"     style=\" width:380px; padding: 10px; \" >\n",
        "As described in lecture, the model function for linear regression (which is a function that maps from `x` to `y`) is represented as\n",
        "\n",
        "$$ f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{1}$$\n",
        "\n",
        "The formula above is how you can represent straight lines - different values of $w$ and $b$ give you different straight lines on the plot. <br/> <br/> <br/> <br/> <br/>\n",
        "\n",
        "Let's try to get a better intuition for this through the code blocks below. Let's start with $w = 100$ and $b = 100$.\n",
        "\n",
        "**Note: You can come back to this cell to adjust the model's w and b parameters**"
      ],
      "metadata": {
        "id": "xTtruV7SIrpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_model_output(x, w, b):\n",
        "    \"\"\"\n",
        "    Computes the prediction of a linear model\n",
        "    Args:\n",
        "      x (ndarray (m,)): Data, m examples\n",
        "      w,b (scalar)    : model parameters\n",
        "    Returns\n",
        "      f_wb (ndarray (m,)): model prediction\n",
        "    \"\"\"\n",
        "    m = x.shape[0]\n",
        "    f_wb = np.zeros(m)\n",
        "    for i in range(m):\n",
        "        f_wb[i] = w * x[i] + b\n",
        "\n",
        "    return f_wb"
      ],
      "metadata": {
        "id": "AVZcGvSCIhMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computing Cost\n",
        "The term 'cost' in this assignment might be a little confusing since the data is housing cost. Here, cost is a measure how well our model is predicting the target price of the house. The term 'price' is used for housing data.\n",
        "\n",
        "The equation for cost with one variable is:\n",
        "  $$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 \\tag{1}$$\n",
        "\n",
        "where\n",
        "  $$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{2}$$\n",
        "  \n",
        "- $f_{w,b}(x^{(i)})$ is our prediction for example $i$ using parameters $w,b$.  \n",
        "- $(f_{w,b}(x^{(i)}) -y^{(i)})^2$ is the squared difference between the target value and the prediction.   \n",
        "- These differences are summed over all the $m$ examples and divided by `2m` (to avoid really big value/simplyfy) to produce the cost, $J(w,b)$.  \n",
        ">Note, in lecture summation ranges are typically from 1 to m, while code will be from 0 to m-1.\n"
      ],
      "metadata": {
        "id": "rxhaLEmKIQlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(x, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the cost function for linear regression.\n",
        "\n",
        "    Args:\n",
        "      x (ndarray (m,)): Data, m examples\n",
        "      y (ndarray (m,)): target values\n",
        "      w,b (scalar)    : model parameters\n",
        "\n",
        "    Returns\n",
        "        total_cost (float): The cost of using w,b as the parameters for linear regression\n",
        "               to fit the data points in x and y\n",
        "    \"\"\"\n",
        "    # number of training examples\n",
        "    m = x.shape[0]\n",
        "\n",
        "    cost_sum = 0\n",
        "    for i in range(m):\n",
        "        f_wb = w * x[i] + b\n",
        "        cost = (f_wb - y[i]) ** 2\n",
        "        cost_sum = cost_sum + cost\n",
        "    total_cost = (1 / (2 * m)) * cost_sum\n",
        "\n",
        "    return total_cost\n",
        ""
      ],
      "metadata": {
        "id": "15N8rCuGHTO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IaCw0X-wKO3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt_intuition(x_train,y_train)"
      ],
      "metadata": {
        "id": "fqNc8I4EKQBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.close('all')\n",
        "fig, ax, dyn_items = plt_stationary(x_train, y_train)\n",
        "updater = plt_update_onclick(fig, ax, x_train, y_train, dyn_items)"
      ],
      "metadata": {
        "id": "idwt7V0iKvKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup_bowl()"
      ],
      "metadata": {
        "id": "Zs9JcnXfKxEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"toc_40291_2.1\"></a>\n",
        "## Gradient descent summary\n",
        "A linear model that predicts $f_{w,b}(x^{(i)})$:\n",
        "$$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{1}$$\n",
        "In linear regression, you utilize input training data to fit the parameters $w$,$b$ by minimizing a measure of the error between our predictions $f_{w,b}(x^{(i)})$ and the actual data $y^{(i)}$. The measure is called the $cost$, $J(w,b)$. In training you measure the cost over all of our training samples $x^{(i)},y^{(i)}$\n",
        "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\\tag{2}$$"
      ],
      "metadata": {
        "id": "MuiZ_sNwJjJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*Gradient descent* was described as:\n",
        "\n",
        "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\n",
        "\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{3}  \\; \\newline\n",
        " b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n",
        "\\end{align*}$$\n",
        "where, parameters $w$, $b$ are updated simultaneously.  \n",
        "The gradient is defined as:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n",
        "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Here *simultaniously* means that you calculate the partial derivatives for all the parameters before updating any of the parameters."
      ],
      "metadata": {
        "id": "TdPkmT1rJlS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"toc_40291_2.2\"></a>\n",
        "## Implement Gradient Descent\n",
        "You will implement gradient descent algorithm for one feature. You will need three functions.\n",
        "- `compute_gradient` implementing equation (4) and (5) above\n",
        "- `compute_cost` implementing equation (2) above (code from previous lab)\n",
        "- `gradient_descent`, utilizing compute_gradient and compute_cost\n",
        "\n",
        "Conventions:\n",
        "- The naming of python variables containing partial derivatives follows this pattern,$\\frac{\\partial J(w,b)}{\\partial b}$  will be `dj_db`.\n",
        "- w.r.t is With Respect To, as in partial derivative of $J(wb)$ With Respect To $b$.\n"
      ],
      "metadata": {
        "id": "YvN1qVL7Jrx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"toc_40291_2.3\"></a>\n",
        "### compute_gradient\n",
        "<a name='ex-01'></a>\n",
        "`compute_gradient`  implements (4) and (5) above and returns $\\frac{\\partial J(w,b)}{\\partial w}$,$\\frac{\\partial J(w,b)}{\\partial b}$. The embedded comments describe the operations."
      ],
      "metadata": {
        "id": "N0-px5zKJthx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient(x, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression\n",
        "    Args:\n",
        "      x (ndarray (m,)): Data, m examples\n",
        "      y (ndarray (m,)): target values\n",
        "      w,b (scalar)    : model parameters\n",
        "    Returns\n",
        "      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w\n",
        "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b\n",
        "     \"\"\"\n",
        "\n",
        "    # Number of training examples\n",
        "    m = x.shape[0]\n",
        "    dj_dw = 0\n",
        "    dj_db = 0\n",
        "\n",
        "    for i in range(m):\n",
        "        f_wb = w * x[i] + b\n",
        "\n",
        "        dj_dw_i = (f_wb - y[i]) * x[i]\n",
        "        dj_db_i = f_wb - y[i]\n",
        "\n",
        "        dj_dw += dj_dw_i\n",
        "        dj_db += dj_db_i\n",
        "\n",
        "    dj_dw = dj_dw / m\n",
        "    dj_db = dj_db / m\n",
        "\n",
        "    return dj_dw, dj_db"
      ],
      "metadata": {
        "id": "2MWA_URiJjiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HA8CKQ3qJvfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt_gradients(x_train,y_train, compute_cost, compute_gradient)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PBOtiFwTKEJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above, the left plot shows $\\frac{\\partial J(w,b)}{\\partial w}$ or the slope of the cost curve relative to $w$ at three points. On the right side of the plot, the derivative is positive, while on the left it is negative. Due to the 'bowl shape', the derivatives will always lead gradient descent toward the bottom where the gradient is zero.\n",
        "\n",
        "The left plot has fixed $b=100$. Gradient descent will utilize both $\\frac{\\partial J(w,b)}{\\partial w}$ and $\\frac{\\partial J(w,b)}{\\partial b}$ to update parameters. The 'quiver plot' on the right provides a means of viewing the gradient of both parameters. The arrow sizes reflect the magnitude of the gradient at that point. The direction and slope of the arrow reflects the ratio of $\\frac{\\partial J(w,b)}{\\partial w}$ and $\\frac{\\partial J(w,b)}{\\partial b}$ at that point.\n",
        "Note that the gradient points *away* from the minimum. Review equation (3) above. The scaled gradient is *subtracted* from the current value of $w$ or $b$. This moves the parameter in a direction that will reduce cost."
      ],
      "metadata": {
        "id": "iU4igdYLKFqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function):\n",
        "    \"\"\"\n",
        "    Performs gradient descent to fit w,b. Updates w,b by taking\n",
        "    num_iters gradient steps with learning rate alpha\n",
        "\n",
        "    Args:\n",
        "      x (ndarray (m,))  : Data, m examples\n",
        "      y (ndarray (m,))  : target values\n",
        "      w_in,b_in (scalar): initial values of model parameters\n",
        "      alpha (float):     Learning rate\n",
        "      num_iters (int):   number of iterations to run gradient descent\n",
        "      cost_function:     function to call to produce cost\n",
        "      gradient_function: function to call to produce gradient\n",
        "\n",
        "    Returns:\n",
        "      w (scalar): Updated value of parameter after running gradient descent\n",
        "      b (scalar): Updated value of parameter after running gradient descent\n",
        "      J_history (List): History of cost values\n",
        "      p_history (list): History of parameters [w,b]\n",
        "      \"\"\"\n",
        "\n",
        "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
        "    J_history = []\n",
        "    p_history = []\n",
        "    b = b_in\n",
        "    w = w_in\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        # Calculate the gradient and update the parameters using gradient_function\n",
        "        dj_dw, dj_db = gradient_function(x, y, w , b)\n",
        "\n",
        "        # Update Parameters using equation (3) above\n",
        "        b = b - alpha * dj_db\n",
        "        w = w - alpha * dj_dw\n",
        "\n",
        "        # Save cost J at each iteration\n",
        "        if i<100000:      # prevent resource exhaustion\n",
        "            J_history.append( cost_function(x, y, w , b))\n",
        "            p_history.append([w,b])\n",
        "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "        if i% math.ceil(num_iters/10) == 0:\n",
        "            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e} \",\n",
        "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
        "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
        "\n",
        "    return w, b, J_history, p_history #return w and J,w history for graphing"
      ],
      "metadata": {
        "id": "vF_wdgXsJztx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Multi-Linear regression"
      ],
      "metadata": {
        "id": "OuNkpevwf-kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
        "y_train = np.array([460, 232, 178])"
      ],
      "metadata": {
        "id": "aRJVMml1f93U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"toc_15456_1.3\"></a>\n",
        "## 1.3 Notation\n",
        "Here is a summary of some of the notation you will encounter, updated for multiple features.  \n",
        "\n",
        "|General <img width=70/> <br />  Notation  <img width=70/> | Description<img width=350/>| Python (if applicable) |\n",
        "|: ------------|: ------------------------------------------------------------||\n",
        "| $a$ | scalar, non bold                                                      ||\n",
        "| $\\mathbf{a}$ | vector, bold                                                 ||\n",
        "| $\\mathbf{A}$ | matrix, bold capital                                         ||\n",
        "| **Regression** |         |    |     |\n",
        "|  $\\mathbf{X}$ | training example matrix                  | `X_train` |   \n",
        "|  $\\mathbf{y}$  | training example  targets                | `y_train`\n",
        "|  $\\mathbf{x}^{(i)}$, $y^{(i)}$ | $i_{th}$Training Example | `X[i]`, `y[i]`|\n",
        "| m | number of training examples | `m`|\n",
        "| n | number of features in each example | `n`|\n",
        "|  $\\mathbf{w}$  |  parameter: weight,                       | `w`    |\n",
        "|  $b$           |  parameter: bias                                           | `b`    |     \n",
        "| $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ | The result of the model evaluation at $\\mathbf{x^{(i)}}$ parameterized by $\\mathbf{w},b$: $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+b$  | `f_wb` |\n"
      ],
      "metadata": {
        "id": "5u1VhThSLPrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"toc_15456_3\"></a>\n"
      ],
      "metadata": {
        "id": "cjLQJjjGLbvZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"toc_15456_3\"></a>\n",
        "# 3 Model Prediction With Multiple Variables\n",
        "The model's prediction with multiple variables is given by the linear model:\n",
        "\n",
        "$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
        "or in vector notation:\n",
        "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$\n",
        "where $\\cdot$ is a vector `dot product`\n",
        "\n",
        "To demonstrate the dot product, we will implement prediction using (1) and (2)."
      ],
      "metadata": {
        "id": "2Nr1x294LdpA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"toc_15456_2.1\"></a>\n",
        "## 2.1 Matrix X containing our examples\n",
        "Similar to the table above, examples are stored in a NumPy matrix `X_train`. Each row of the matrix represents one example. When you have $m$ training examples ( $m$ is three in our example), and there are $n$ features (four in our example), $\\mathbf{X}$ is a matrix with dimensions ($m$, $n$) (m rows, n columns).\n",
        "\n",
        "\n",
        "$$\\mathbf{X} =\n",
        "\\begin{pmatrix}\n",
        " x^{(0)}_0 & x^{(0)}_1 & \\cdots & x^{(0)}_{n-1} \\\\\n",
        " x^{(1)}_0 & x^{(1)}_1 & \\cdots & x^{(1)}_{n-1} \\\\\n",
        " \\cdots \\\\\n",
        " x^{(m-1)}_0 & x^{(m-1)}_1 & \\cdots & x^{(m-1)}_{n-1}\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "notation:\n",
        "- $\\mathbf{x}^{(i)}$ is vector containing example i. $\\mathbf{x}^{(i)}$ $ = (x^{(i)}_0, x^{(i)}_1, \\cdots,x^{(i)}_{n-1})$\n",
        "- $x^{(i)}_j$ is element j in example i. The superscript in parenthesis indicates the example number while the subscript represents an element.  \n",
        "\n",
        "Display the input data."
      ],
      "metadata": {
        "id": "iJIzVeqBLhUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"toc_15456_2.2\"></a>\n",
        "## 2.2 Parameter vector w, b\n",
        "\n",
        "* $\\mathbf{w}$ is a vector with $n$ elements.\n",
        "  - Each element contains the parameter associated with one feature.\n",
        "  - in our dataset, n is 4.\n",
        "  - notionally, we draw this as a column vector\n",
        "\n",
        "$$\\mathbf{w} = \\begin{pmatrix}\n",
        "w_0 \\\\\n",
        "w_1 \\\\\n",
        "\\cdots\\\\\n",
        "w_{n-1}\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "* $b$ is a scalar parameter.  "
      ],
      "metadata": {
        "id": "7pemisIULmGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"toc_15456_4\"></a>\n",
        "# 4 Compute Cost With Multiple Variables\n",
        "\n",
        "The cost for that example  $$cost^{(i)} =  (f_{wb} - y^{(i)})^2$$\n",
        "\n",
        "The equation for the cost function with multiple variables $J(\\mathbf{w},b)$ is:\n",
        "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$\n",
        "where:\n",
        "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{4} $$\n",
        "\n",
        "\n",
        "In contrast to previous labs, $\\mathbf{w}$ and $\\mathbf{x}^{(i)}$ are vectors rather than scalars supporting multiple features."
      ],
      "metadata": {
        "id": "NY6IKQXKL1wB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(X, y, w, b):\n",
        "    \"\"\"\n",
        "    compute cost\n",
        "    Args:\n",
        "      X (ndarray (m,n)): Data, m examples with n features\n",
        "      y (ndarray (m,)) : target values\n",
        "      w (ndarray (n,)) : model parameters\n",
        "      b (scalar)       : model parameter\n",
        "\n",
        "    Returns:\n",
        "      cost (scalar): cost\n",
        "    \"\"\"\n",
        "    m = X.shape[0]\n",
        "    cost = 0.0\n",
        "    for i in range(m):\n",
        "        f_wb_i = np.dot(X[i], w) + b           #(n,)(n,) = scalar (see np.dot)\n",
        "        cost = cost + (f_wb_i - y[i])**2       #scalar\n",
        "    cost = cost / (2 * m)                      #scalar\n",
        "    return cost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "dKaO0At7LP_E",
        "outputId": "308e7645-6168-411c-da53-a87291a00ba2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-2-b48193222fe0>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-b48193222fe0>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    <a name=\"toc_15456_4\"></a>\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"toc_15456_5\"></a>\n",
        "# 5 Gradient Descent With Multiple Variables\n",
        "Gradient descent for multiple variables:\n",
        "\n",
        "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
        "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{for j = 0..n-1}\\newline\n",
        "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
        "\\end{align*}$$\n",
        "\n",
        "where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where  \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{7}\n",
        "\\end{align}\n",
        "$$\n",
        "* m is the number of training examples in the data set\n",
        "\n",
        "    \n",
        "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value\n"
      ],
      "metadata": {
        "id": "O4ndJYr6L6Kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Performs batch gradient descent to learn w and b. Updates w and b by taking\n",
        "    num_iters gradient steps with learning rate alpha\n",
        "\n",
        "    Args:\n",
        "      X (ndarray (m,n))   : Data, m examples with n features\n",
        "      y (ndarray (m,))    : target values\n",
        "      w_in (ndarray (n,)) : initial model parameters\n",
        "      b_in (scalar)       : initial model parameter\n",
        "      cost_function       : function to compute cost\n",
        "      gradient_function   : function to compute the gradient\n",
        "      alpha (float)       : Learning rate\n",
        "      num_iters (int)     : number of iterations to run gradient descent\n",
        "\n",
        "    Returns:\n",
        "      w (ndarray (n,)) : Updated values of parameters\n",
        "      b (scalar)       : Updated value of parameter\n",
        "      \"\"\"\n",
        "\n",
        "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
        "    J_history = []\n",
        "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
        "    b = b_in\n",
        "\n",
        "    for i in range(num_iters):\n",
        "\n",
        "        # Calculate the gradient and update the parameters\n",
        "        dj_db,dj_dw = gradient_function(X, y, w, b)   ##None\n",
        "\n",
        "        # Update Parameters using w, b, alpha and gradient\n",
        "        w = w - alpha * dj_dw               ##None\n",
        "        b = b - alpha * dj_db               ##None\n",
        "\n",
        "        # Save cost J at each iteration\n",
        "        if i<100000:      # prevent resource exhaustion\n",
        "            J_history.append( cost_function(X, y, w, b))\n",
        "\n",
        "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "        if i% math.ceil(num_iters / 10) == 0:\n",
        "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
        "\n",
        "    return w, b, J_history #return final w,b and J history for graphing"
      ],
      "metadata": {
        "id": "brdc-4ECLxke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize parameters\n",
        "initial_w = np.zeros_like(w_init)\n",
        "initial_b = 0.\n",
        "# some gradient descent settings\n",
        "iterations = 1000\n",
        "alpha = 5.0e-7\n",
        "# run gradient descent\n",
        "w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,\n",
        "                                                    compute_cost, compute_gradient,\n",
        "                                                    alpha, iterations)\n",
        "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
        "m,_ = X_train.shape\n",
        "for i in range(m):\n",
        "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
      ],
      "metadata": {
        "id": "r0bNXD3LNqQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### z-score normalization\n",
        "After z-score normalization, all features will have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "To implement z-score normalization, adjust your input values as shown in this formula:\n",
        "$$x^{(i)}_j = \\dfrac{x^{(i)}_j - \\mu_j}{\\sigma_j} \\tag{4}$$\n",
        "where $j$ selects a feature or a column in the $\\mathbf{X}$ matrix. $µ_j$ is the mean of all the values for feature (j) and $\\sigma_j$ is the standard deviation of feature (j).\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mu_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} x^{(i)}_j \\tag{5}\\\\\n",
        "\\sigma^2_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} (x^{(i)}_j - \\mu_j)^2  \\tag{6}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        ">**Implementation Note:** When normalizing the features, it is important\n",
        "to store the values used for normalization - the mean value and the standard deviation used for the computations. After learning the parameters\n",
        "from the model, we often want to predict the prices of houses we have not\n",
        "seen before. Given a new x value (living room area and number of bed-\n",
        "rooms), we must first normalize x using the mean and standard deviation\n",
        "that we had previously computed from the training set.\n",
        "\n",
        "**Implementation**"
      ],
      "metadata": {
        "id": "CvTdh1tdMPZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def zscore_normalize_features(X):\n",
        "    \"\"\"\n",
        "    computes  X, zcore normalized by column\n",
        "\n",
        "    Args:\n",
        "      X (ndarray (m,n))     : input data, m examples, n features\n",
        "\n",
        "    Returns:\n",
        "      X_norm (ndarray (m,n)): input normalized by column\n",
        "      mu (ndarray (n,))     : mean of each feature\n",
        "      sigma (ndarray (n,))  : standard deviation of each feature\n",
        "    \"\"\"\n",
        "    # find the mean of each column/feature\n",
        "    mu     = np.mean(X, axis=0)                 # mu will have shape (n,)\n",
        "    # find the standard deviation of each column/feature\n",
        "    sigma  = np.std(X, axis=0)                  # sigma will have shape (n,)\n",
        "    # element-wise, subtract mu for that column from each example, divide by std for that column\n",
        "    X_norm = (X - mu) / sigma\n",
        "\n",
        "    return (X_norm, mu, sigma)\n",
        "\n",
        "#check our work\n",
        "#from sklearn.preprocessing import scale\n",
        "#scale(X_orig, axis=0, with_mean=True, with_std=True, copy=True)"
      ],
      "metadata": {
        "id": "LJ3K9zrbMP0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  "
      ],
      "metadata": {
        "id": "LKG6s_txMRrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cost functions with regularization\n",
        "### Cost function for regularized linear regression\n",
        "\n",
        "The equation for the cost function regularized linear regression is:\n",
        "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2  + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 \\tag{1}$$\n",
        "where:\n",
        "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{2} $$\n",
        "\n",
        "\n",
        "Compare this to the cost function without regularization (which you implemented in  a previous lab), which is of the form:\n",
        "\n",
        "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 $$\n",
        "\n",
        "The difference is the regularization term,  <span style=\"color:blue\">\n",
        "    $\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$ </span>\n",
        "    \n",
        "Including this term encourages gradient descent to minimize the size of the parameters. Note, in this example, the parameter $b$ is not regularized. This is standard practice.\n",
        "\n",
        "Below is an implementation of equations (1) and (2). Note that this uses a *standard pattern for this course*,   a `for loop` over all `m` examples."
      ],
      "metadata": {
        "id": "VSJKfm5ilUL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost_linear_reg(X, y, w, b, lambda_ = 1):\n",
        "    \"\"\"\n",
        "    Computes the cost over all examples\n",
        "    Args:\n",
        "      X (ndarray (m,n): Data, m examples with n features\n",
        "      y (ndarray (m,)): target values\n",
        "      w (ndarray (n,)): model parameters\n",
        "      b (scalar)      : model parameter\n",
        "      lambda_ (scalar): Controls amount of regularization\n",
        "    Returns:\n",
        "      total_cost (scalar):  cost\n",
        "    \"\"\"\n",
        "\n",
        "    m  = X.shape[0]\n",
        "    n  = len(w)\n",
        "    cost = 0.\n",
        "    for i in range(m):\n",
        "        f_wb_i = np.dot(X[i], w) + b                                   #(n,)(n,)=scalar, see np.dot\n",
        "        cost = cost + (f_wb_i - y[i])**2                               #scalar\n",
        "    cost = cost / (2 * m)                                              #scalar\n",
        "\n",
        "    reg_cost = 0\n",
        "    for j in range(n):\n",
        "        reg_cost += (w[j]**2)                                          #scalar\n",
        "    reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar\n",
        "\n",
        "    total_cost = cost + reg_cost                                       #scalar\n",
        "    return total_cost                                                  #scalar"
      ],
      "metadata": {
        "id": "KunWxj6KlUfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient descent with regularization\n",
        "The basic algorithm for running gradient descent does not change with regularization, it is:\n",
        "$$\\begin{align*}\n",
        "&\\text{repeat until convergence:} \\; \\lbrace \\\\\n",
        "&  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1} \\\\\n",
        "&  \\; \\; \\;  \\; \\;b = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\\\\n",
        "&\\rbrace\n",
        "\\end{align*}$$\n",
        "Where each iteration performs simultaneous updates on $w_j$ for all $j$.\n",
        "\n",
        "What changes with regularization is computing the gradients."
      ],
      "metadata": {
        "id": "rNk9fZ4ClfPj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computing the Gradient with regularization (both linear/logistic)\n",
        "The gradient calculation for both linear and logistic regression are nearly identical, differing only in computation of $f_{\\mathbf{w}b}$.\n",
        "$$\\begin{align*}\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  +  \\frac{\\lambda}{m} w_j \\tag{2} \\\\\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{3}\n",
        "\\end{align*}$$\n",
        "\n",
        "* m is the number of training examples in the data set      \n",
        "* $f_{\\mathbf{w},b}(x^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target\n",
        "\n",
        "      \n",
        "* For a  <span style=\"color:blue\"> **linear** </span> regression model  \n",
        "    $f_{\\mathbf{w},b}(x) = \\mathbf{w} \\cdot \\mathbf{x} + b$  \n",
        "* For a <span style=\"color:blue\"> **logistic** </span> regression model  \n",
        "    $z = \\mathbf{w} \\cdot \\mathbf{x} + b$  \n",
        "    $f_{\\mathbf{w},b}(x) = g(z)$  \n",
        "    where $g(z)$ is the sigmoid function:  \n",
        "    $g(z) = \\frac{1}{1+e^{-z}}$   \n",
        "    \n",
        "The term which adds regularization is  the <span style=\"color:blue\">$\\frac{\\lambda}{m} w_j $</span>."
      ],
      "metadata": {
        "id": "IU1p4cOklgsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient_linear_reg(X, y, w, b, lambda_):\n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression\n",
        "    Args:\n",
        "      X (ndarray (m,n): Data, m examples with n features\n",
        "      y (ndarray (m,)): target values\n",
        "      w (ndarray (n,)): model parameters\n",
        "      b (scalar)      : model parameter\n",
        "      lambda_ (scalar): Controls amount of regularization\n",
        "\n",
        "    Returns:\n",
        "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.\n",
        "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b.\n",
        "    \"\"\"\n",
        "    m,n = X.shape           #(number of examples, number of features)\n",
        "    dj_dw = np.zeros((n,))\n",
        "    dj_db = 0.\n",
        "\n",
        "    for i in range(m):\n",
        "        err = (np.dot(X[i], w) + b) - y[i]\n",
        "        for j in range(n):\n",
        "            dj_dw[j] = dj_dw[j] + err * X[i, j]\n",
        "        dj_db = dj_db + err\n",
        "    dj_dw = dj_dw / m\n",
        "    dj_db = dj_db / m\n",
        "\n",
        "    for j in range(n):\n",
        "        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n",
        "\n",
        "    return dj_db, dj_dw"
      ],
      "metadata": {
        "id": "Ihc-ED2xlj6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4QQicSqqlWMr"
      }
    }
  ]
}